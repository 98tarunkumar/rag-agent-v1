# Specialist Agent - RAG-based AI Assistant

A Retrieval-Augmented Generation (RAG) application using Ollama and ChromaDB for document processing and question answering.

## âœ… Error Fixed!

The ES module error has been resolved. All files now use CommonJS syntax (`require`/`module.exports`) consistently.

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
npm install
```

### 2. Install and Start Ollama
```bash
# Install Ollama (if not already installed)
# Visit: https://ollama.ai/download

# Start Ollama server
ollama serve

# Pull required models (in a new terminal)
ollama pull nomic-embed-text
ollama pull llama3.2:3b
```

### 3. Install and Start ChromaDB
```bash
# Install ChromaDB
pip install chromadb

# Start ChromaDB server
chroma run --host localhost --port 8000
```

### 4. Add Your Documents
Place your domain-specific documents in the `data/documents/` folder.

### 5. Run the Application
```bash
npm start
```

## ğŸ“š API Endpoints

- **`GET /`** - API documentation
- **`GET /api/health`** - Health check
- **`POST /api/query`** - Ask questions to your specialist agent
- **`POST /api/ingest`** - Ingest documents from a directory

## ğŸ”§ Usage Examples

### Ingest Documents
```bash
curl -X POST http://localhost:3000/api/ingest \
  -H "Content-Type: application/json" \
  -d '{"directoryPath": "./data/documents"}'
```

### Query the Agent
```bash
curl -X POST http://localhost:3000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic of the documents?"}'
```

## ğŸ› ï¸ Configuration

Update `.env` file with your settings:

```env
# Server Configuration
PORT=3000
NODE_ENV=development

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=llama3.2:3b

# Vector Store Configuration (ChromaDB)
CHROMA_URL=http://localhost:8000
COLLECTION_NAME=specialist-agent

# Optional: Additional Configuration
MAX_FILE_SIZE=10mb
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

## ğŸ“ Project Structure

```
specialist-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ embedding.js      # Ollama embeddings
â”‚   â”‚   â”œâ”€â”€ vectorstore.js    # ChromaDB integration
â”‚   â”‚   â””â”€â”€ chat.js          # Ollama chat service
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ documentLoader.js # Document processing
â”‚   â”‚   â””â”€â”€ textSplitter.js   # Text chunking
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ api.js           # API endpoints
â”‚   â””â”€â”€ app.js               # Main application
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/           # Your domain documents
â”œâ”€â”€ package.json            # Dependencies
â”œâ”€â”€ .env                    # Configuration
â””â”€â”€ README.md              # This file
```

## ğŸ¯ Features

- **Document Processing**: Supports PDF, DOCX, TXT, and Markdown files
- **Local AI**: Uses Ollama for embeddings and chat (no API keys needed)
- **Vector Storage**: ChromaDB for efficient similarity search
- **RAG Pipeline**: Retrieval-Augmented Generation for context-aware responses
- **Specialist Focus**: Optimized for domain-specific knowledge bases

## ğŸ” Troubleshooting

### ChromaDB Connection Error
Make sure ChromaDB server is running:
```bash
chroma run --host localhost --port 8000
```

### Ollama Connection Error
Make sure Ollama server is running:
```bash
ollama serve
```

### Model Not Found
Pull the required models:
```bash
ollama pull nomic-embed-text
ollama pull llama3.2:3b
```

## ğŸ“ License

MIT License - feel free to use this project for your own specialist agents!
